{
  
    
        "post0": {
            "title": "Demand Forecasting with Bayesian Methods",
            "content": "The Stanford Medical system sees a constant stream of patients through its doors every year, many of whom require intensive care. Blood transfusions are a common occurrence as part of a therapy or surgery, but blood platelets are a scarce resource that have a short shelf life. Therefore accurately predicting future demand is essential in order to minimize waste while ensuring hospitals have enough on hand for the current patient population. In 2017, Guan et al. published an initial model employing a frequentist approach, minimizing waste in a linear program with constraints that took patient features and current stores in account. This model was eventually rolled out into production in an open source R package titled SBCpip and currently serves the medical system. . I investigated two alternative Bayesian methods to predict demand more transparently: online regression and Gaussian processes regression. Bayesian models offer a number of key advantages over the current system. First, while the linear program approach outputs simple point estimates for the optimal inventory to have on hand, these Bayesian procedures generate a richer posterior distribution that can be used to quantify uncertainty and quantify how the model inputs impact predictions. Second, the data is messy and manually extracted into excel files, leading to duplicates and nonsensical values. The methods explored offer a higher degree of robustness to outliers, which can be tuned by varying the prior distributions. Third, since other hospital systems have similar blood platelet demands, there have been some efforts to generalize SBCpip to other facilities. The Bayesian approaches considered would allow models to lean more heavily on priors before adequate data has been generated and become more customized to each facility over time as data accumulates. Fourth, the linear program approach predicts demand indirectly, making results much harder to interpret. The methods proposed here allow for greater flexibility and transparency in diagnosing issues and determining the underlying drivers of a particular prediction. . Approaches . While the data cannot be made publicly available, there are a few key features to note. Our outcome variable, blood platelet units used, helps determine the best types of models to use for this problem. The amount used is a real valued number that closely follows a Gaussian curve, permitting the use of a rich class of normally distributed models. The regular fluctuations suggest that some seasonality may be present that could be picked up by a Gaussian process regression model given a suitable kernel. Below I consider some plausible Bayesian models that each have tradeoffs in simplicity, accuracy, and utility. . Online Regression . In a typical Bayesian regression problem, we have a setting with independent and identically distributed training data with future observations drawn from the same population. In this case we can simply place a prior over the weights and variance, evaluate the data likelihood and find the posterior through exact or approximate inference to determine a stable model from which we can make predictions. Here we are presented with a time series of features and inventory data and want our model to flexibly model trends that depend on our time step. Online regression is a naive baseline for this problem in which at time step t, we take the posterior from time step t − 1 as our prior and update the model using only the likelihood over the most recent batch of data. Define our original priors as $p( theta_0)$, then our model updates occur sequentially: . begin{align} p( theta_{t}) = p( theta_{t} mid y_{t-1}, x_{t-1}, theta_{t-1}) &amp; propto p( y_{t-1} mid x_{t-1}, theta_{t-1})p( theta_{t-1}) p( theta_{t+1} mid y_{t}, x_{t}, theta_{t}) &amp; propto p( y_{t} mid x_{t}, theta_{t})p( theta_{t}) end{align} . While a simple regression may seem underpowered for this task, it provides an excellent baseline against which we can compare more complex approaches. The conjugacy of the Gaussian model makes it extremely efficient as we add new data points with easy interpretation. However, its failure to model dependency between data points may introduce bias that could be avoided with more complex approaches. . The model as implemented closely follows the approach outlined in Christopher Bishop’s Pattern Recognition and Machine Learning. Given model $y_i = w_ix_i^T + varepsilon_i$, we give likelihood and prior distributions as $p left(y_{i} mid x_{i}, w_{i} right)= mathcal{N} left(w_{i} x_{i}^{ top}, beta^{-1} right)$ and $p left(w_{0} right)= mathcal{N} left(m_{0}, S_{0} right)$ respectively, where $ beta$ is a precision hyperparameter, $m_0 = mathbf{0}$ and $S_0 = diag( alpha^{-1})$ for hyperparameter $ alpha$. Our posterior is analytically derived as begin{align} p left(w_{i+1} mid w_{i}, x_{i}, y_{i} right)= mathcal{N} left(m_{i+1}, S_{i+1} right) S_{i+1}= left(S_{i}^{-1}+ beta x_{i}^{ top} x_{i} right)^{-1} m_{i+1}=S_{i+1} left(S_{i}^{-1} m_{i}+ beta x_{i} y_{i} right) end{align} . For model criticism, Bishop recommends evaluating the marginal likelihood, or model evidence, which he derives analytically. For M features and N observations, the log marginal likelihood can be expressed as begin{align} log p({y} mid alpha, beta)= frac{M}{2} log alpha+ frac{N}{2} log beta-E left({m}_{N} right)- frac{1}{2} log left|{S}_{N}^{-1} right|- frac{N}{2} log 2 pi end{align} Therefore we can directly use this expression to select for optimal hyperparameters $ alpha, beta$ to improve the model fit. . Gaussian Processes Regression . I look to Gaussian processes (GP) regression to correct for the deficiencies of the regression model. While the prior model considered datapoints to be independently drawn from a stationary distribution, Gaussian processes explicitly model the covariance relations between time steps. Here we consider our feature vectors $x in R^d$ over T time steps $x_1,…,x_T$ and each observation of inventory used $y_i = f(x_i)$. If we assume the functions $f$ to be given by a prior Gaussian process $f sim GP( mu, K)$ for some mean $ mu$ and covariance matrix $K$ and $y_i sim N(f(x_i), sigma^2)$ then we obtain posterior $p left({f} mid x_{n}, y_{n} right) propto {N} left({f} mid { mu}^{ prime}, {K}^{ prime} right)$ for ${K}^{ prime}= left({K}^{-1}+ sigma^{-2} {I} right)^{-1}, ; { mu}^{ prime}={K}^{ prime} left({K}^{-1} { mu}+ sigma^{-2} {y} right)$. Similarly, we can obtain an exact posterior predictive distribution given by begin{align} y_{T+1} | x_{T+1}, x_i, y_i &amp; sim N(m_{T+1}, v_{T+1}) m_{N+1} &amp;= mu left({x}_{N+1} right)+{k}^{ top} left({K}+ sigma^{2} {I} right)^{-1}({y}-{ mu}) v_{N+1} &amp;=K left({x}_{N+1}, {x}_{N+1} right)-{k}^{ top} left({K}+ sigma^{2} {I} right)^{-1} {k}+ sigma^{2} end{align} . GP regression offers additional modeling decisions that improve its attractiveness for our use case. First, there are a variety of potential kernels for expressing the relationship between feature vectors over time. Our choice determines how closely we model regular periodicity and linear trends in the data. . Additionally, since GP regression is closely related to time series methods like ARIMA, we can eschew using features at all and treat the blood inventory as a standalone data series. Why might we prefer this approach? The other regression approaches obtain a posterior predictive distribution by conditioning on $x_{N+i}$ for $i in [1,7]$, feature vectors which are supposed to contain data like patient counts and other measures that aren’t actually available for future days. While features could be extrapolated from current data, a GP regression approach can condition simply on the time step and avoid some of these messier modeling decisions. . Experiments . Over the experiments run, the simple online regression obtained the lowest mean absolute error across 1, 3, and 7 day prediction intervals. This result is somewhat surprising given the difference between the modeling assumptions (eg. independence) and the true data generating process but can be reconciled when we consider that we limited GP regression’s performance by limiting the features used in fitting. While online regression achieved the best results under the evaluation metric, GP regression may still be preferable in a practical setting. Comparing the mean absolute error among the methods explored: . begin{array}{l|cccc} Model &amp; MAE , 1 , Day &amp; MAE , 3 ,Day &amp; MAE , 7 ,Day Online Regression &amp; 6.40 &amp; 6.65 &amp; 6.58 GP Regression &amp; 7.33 &amp; 7.80 &amp; 7.89 end{array} . Online Regression . The regressions were tested under two scenarios: for each time period, we could either fit the model using data since inception or look back over a fixed window. The data since inception approach was too constraining and performed slightly worse in evaluations, leading the fixed window approach to be used for comparisons. The model was fit over 7 day increments and produced predictions for 7 days into the future. . We can understand the performance of the model by looking at the results more closely. First, while the online regression approach does not explicitly model a time series, one-hot encoded day of the week features are included as input to the model. Therefore it still can capture some of the cyclicality of blood platelet demand. . The online regression was fit over a grid of potential hyperparameter values for $ alpha, beta$, with the model with the highest marginal likelihood selected for final predictions. It was sensitive to extreme values of each hyperparameter, but otherwise relatively robust to misspecification within a neighborhood of values. . Gaussian Processes Regression . GP regression offers a variety of modeling decisions, but perhaps the most important is the choice of kernel. This decision directly impacts the model’s ability to reflect the true relationship between observations. In “Automatic model construction with Gaussian processes” David Duvenaud outlines a decision process for choosing the best kernel given a dataset. I experimented with permutations of additive kernels, combining Exponential Quadratic, Linear, Periodic, and Locally Periodic varieties. The Locally Periodic kernel was of particular interest as the data shows consistent day of the week seasonality but with magnitudes that vary over time. . The model was fit in TensorFlow Probability by maximizing the marginal likelihood of the model parameters and the observation noise variance over the training data via gradient descent. The GP regression model was then fit on the data using the optimized kernel before posterior samples were drawn. Over all kernel experiments, a combination kernel of Exponential Quadratic and Locally Periodic varieties produces the best results. . . The model was then fit for periods increasing in 7 day increments from the data series inception, with predictions made for the following 7 day increment. For example at day 100, data from days 0-100 are taken as the training set, with the mean and standard deviation from the posterior predictive distribution forming confidence intervals for the prediction period. Alternatively, samples could be drawn from the posterior predictive distribution to simulate different paths created by the Gaussian process. . . We can make further sense of GP regression’s underperformance by looking at the kernels learned themselves. Comparing kernels fit in the training process along with the true observations, we can see that given little data, the kernel is unable to adapt to the variation in the data. A kernel with parameters trained on the full dataset shows much more reasonable variation, with both global and local trends. However, comparing this kernel to the observed variation, we can see it still has a long way to go to match the ground truth. Additional data would go a long way to improving this method, but there may also be more expressive kernels than those explored that could drastically improve performance. . While the hyperparameter search was partially limited by compute time, GP regression’s results indicate the method could show promise in this application. While it achieved higher error than other methods, its performance could improve if we included additional features, some of which might be known ahead of time by hospital administrators. Additional variance control, such as full marginalization of the model’s hyperparameters, would also increase its effectiveness. . Conclusions . I took a survey approach to understanding the tradeoffs among Bayesian methods for resolving a difficult, real-world problem. Forecasting in general is a daunting task, since it requires extending a model beyond the domain of observations seen. In the medical setting, it is even more crucial to have humans in the loop, and while the original linear program could set hard boundaries on the predictions produced, it is more useful to ensure that end users have an intuitive understanding of the results produced and the tools to probe alternative outcomes. . The Bayesian methods explored each had significant problems in completely satisfying the objectives set out in the introduction, but the approaches could be considered a starting point for additional refinement. In particular, GP regression’s ability to generate sample trajectories and credible intervals could prove valuable for those in charge of inventory ordering decisions. Quantified uncertainty can be vastly superior to relying on heuristics. . Finally, the experiments made clear that the data may not contain sufficient information for more exact predictions. Some effort should be expended to encourage the medical system to maintain more expansive records, enabling models to find the best features that drive blood platelet demand. .",
            "url": "https://spencerbraun.github.io/blog/bayesian/machine-learning/python/statistics/projects/tensorflow/2021/07/22/demand-forecasting-with-bayesian-methods.html",
            "relUrl": "/bayesian/machine-learning/python/statistics/projects/tensorflow/2021/07/22/demand-forecasting-with-bayesian-methods.html",
            "date": " • Jul 22, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Chinese Word Segmentation: Classic and Modern Methods",
            "content": "Part of the fun of NLP is the diversity of language structures that models must consider. Tools that are built in a monolingual context can easily fail to achieve more global utility. If you just work in English, you may find word embeddings fun and useful, but the German Komposita don’t fit neatly into our English-centric boxes. Similarly, once we leave the Latin alphabet, we acquire all sorts of new challenges. . Chinese sentences do not typically contain spaces to separate words. From context it may be clear which characters combine to form distinct words, and finding ways to endow this ability to a model is a classic NLP task. I looked to see how well classic machine learning algorithms could learn to accurately segment words and then explored some more modern methods that can also offer a richer set of labels. . Binary Classification . In “A realistic and robust model for Chinese word segmentation”, Chu-Ren Huang and co-authors treat segmentation as a binary classification task, predicting whether a word separation exists between two characters or not. This approach has obvious appeal, as it reduces a seemingly complex problem into a simple task. . They examine the efficacy of a number of classifiers, including LDA, logistic regression, SVMs, and feed-forward neural nets. While there are some performance differences among them, the real work involved stems from engineering the features that are input into these models. This is the common refrain for modeling language before deep learning subsumed the field and could easily learn in an end-to-end fashion. . First, we must find adjacent characters for the model to classify, but we likely also want some context since just two characters may not be enough information even for a native speaker. Huang et al. construct 5 features per sample such that for a character string like [ABCD] we end up with [AB, B, BC, C, CD]. For maximal data efficiency, we want this for every pair of adjacent characters in the training set. I’m working with a dataset with spaces inserted into sentences to show the true word breaks, so my task is to both featurize and label this dataset. I constructed the samples by passing a sliding 4-character window over each sentence: {python, eval=F} def create_samples(sentence: str) -&gt; str: “”” Breaks passed sentence into all combinations of 4 adjacent characters using a sliding window over the sentence. Creates label 1 if word break exists in the middle of 4 characters, 0 if not. “”” . split_line = sentence.strip().split(&quot; &quot;) line_breaks = [len(x) for x in split_line] cum_breaks = [sum(line_breaks[0:x]) for x in range(1, len(line_breaks))] offset = 0 samples = [] for idx in range(len(split_line) - 1): curr_len = len(split_line[idx]) string = &quot;&quot;.join(split_line)[offset : offset + curr_len + 3] if len(string) &lt; 4: break for pos in range(curr_len): sample = string[pos : pos + 4] if len(sample) &lt; 4: break label = 1 if (offset + pos + 2) in cum_breaks else 0 samples.append((sample, label)) offset += curr_len return samples . This system captured all consecutive strings in a sentence, producing 2,638,057 strings with a word break in the center and 1,591,532 strings without a center word break in the training set. While this dataset is somewhat imbalanced, the test set had a very similar balance post-processing. I then split each sample into the 5-feature format {python, eval=F} def featurize(sample: Tuple[List[str], int]) -&gt; tuple: “”” Given a sample tuple of a 4 character chinese string and a label, (‘ABCD’, 1), returns featurized version of the form ((‘AB’, ‘B’, ‘BC’, ‘C’, ‘CD’), 1) “”” chars, label = sample features = (chars[:2], chars[1], chars[1:3], chars[2], chars[2:]) . return features, label . Now we have distinct samples and features, but we still aren’t ready for modeling. An SVM doesn’t place a hyperplane in the space of Chinese characters - we need numbers. In the deep learning space, we would be ready to assign dense word and position embeddings, but here we will turn to one-hot vectors. To construct we follow the steps: . Each character is assigned an integer index | For each feature string in a sample, we take a zero vector of dimension equal to the size of the vocabulary and fill in 1 in the index corresponding to the characters in the string. | Concatenate all five vectors from each feature to form a very long, sparse vector for each sample | . I played with some additional changes, such as separate indexing of unigrams and bigrams, but the process as outlined is the basic idea. We still have one final data challenge: our vectors are incredibly sparse. Each sample is composed of a vector containing 7 1’s and thousands of 0’s. Here we turn to SciPy’s csr_matrix, specifically designed to store only the non-zero values of matrix and fully compatible with scikit-learn algorithms. . Results . While I considered many possible modeling techniques, I focused on support vector machines for two reasons. First, the compressed sparse row format of the one-hot encoded matrix offered significant memory and computation speed advantages given an algorithm that could work with this sparsity. Ultimately, discriminative algorithms met this criteria while generative models like LDA required the full dataset to parameterize its Gaussian distributions. Second, SVMs are simple and given their high performance, there was no need to delve into more complex options that could be harder to maintain. . SVMs offer high degrees of flexibility by using the kernel trick to transform the data space and increase class separation. Due to the high dimensionality of the data, classes were likely to be close to separable and empirical testing with non-linear kernels did not improve fit. Data scaling also negatively impacted scores in cross-validation. . On a held out test dataset, the SVM had an accuracy of 97.6% and an F1 score of 98%. This is pretty impressive and reminds us that reaching for simple tools first can save us a lot of time and compute. . I was curious what patterns I could find in the test samples that assigned incorrect labels. Of the 12,137 samples used in the test set, the SVM misclassified 293 of them. For 69 of these samples, the center bigram was not present in the training dataset. This is important since the model was predicting whether a separation exists between these two characters, but each of these bigrams would be assigned the same “unknown” placeholder value under the feature construction process. This is a general problem of using a fixed vocabulary that a more complex tokenization process could ameliorate. . Transformer Architectures . Turning to more modern methods, I flipped to Sebastian Ruder’s NLP Progress leaderboard for Chinese word segmentation. The top scoring paper on the Chinese Treebank 6 is “Towards Fast and Accurate Neural Chinese Word Segmentation with Multi-Criteria Learning” by Weipeng Huang et al. from Ant Group. Their methods combine a Chinese-language BERT model with additional projection layers, knowledge distillation, and quantization. I was more interested in seeing how a Transformer approach to the problem might differ, so I implemented a paired down version of this method. . The Ant Group paper also takes a different labeling approach, seeking to categorize each token as the beginning, middle, end, or single character of words in a sentence. To follow their scheme, the separated words in the training and test sets were concatenated to remove spaces, and each character was given a label in “B”, “M”, “E”, and “S” corresponding to beginning, middle, end, or single respectively. . I loaded a pre-trained BERT Encoder from Hugging Face and added a dropout and linear layer projecting to the space of possible classes. The labeling tokens were added to the vocabulary and the number of classes easily specified: . {python, eval=F} from transformers import BertTokenizerFast, BertForTokenClassification . tokenizer = BertTokenizerFast.from_pretrained( “bert-base-chinese”, cache_dir=cache_dir ) tokenizer.add_tokens([“A”, “S”, “B”, “M”, “E”]) . model = BertForTokenClassification.from_pretrained( “bert-base-chinese”, cache_dir=cache_dir ) model.classifier = torch.nn.Linear(768, 5, bias=True) model.num_labels = 5 . The model was fine-tuned on the provided segmentation dataset and constructed labels until convergence, as measured by the loss on a small validation set carved out of the training data. The BERT experiments offered many avenues for optimization and only a small subset were attempted for now. The main ablations focused on which parameters were trained; nested versions were run training just the linear classification layer, adding the last three BERT encoder layers, and training all parameters. The results demonstrated that including some encoding layers made a large difference. . I measured the Encoder classification model along two metrics; accuracy is the percent of test samples for which the encoder produced the correct label for every token, while “% Correct Tokens” is the percent of all tokens that received the correct label. My simple BERT implementation scored 74% accuracy and 97% on the “% Correct Tokens” metric. The model showed promising performance on this harder task and would likely benefit from additional ablations. I plan to work on improving training methods, swapping in more modern Encoders than BERT, and adding new projection layers. .",
            "url": "https://spencerbraun.github.io/blog/nlp/machine-learning/projects/pytorch/2021/07/20/chinese-segmentation-classic-and-modern-methods.html",
            "relUrl": "/nlp/machine-learning/projects/pytorch/2021/07/20/chinese-segmentation-classic-and-modern-methods.html",
            "date": " • Jul 20, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Selectively Editable Language Models",
            "content": "The post belows explains a project in editing language models I completed as part of Stanford’s course in natural language processing. If you want to skip to the point, the report and code are freely available. . Background: Taming Large Language Models . Large language models have been an area of exciting development in the last few years, and we now have applications and entire companies built around their improving performance and utility. The number of pretrained models hosted on Hugging Face has exploded, companies built on GPT-3’s API are rolling out betas, and GitHub Copilot amazes senior ML researchers (though not without controversies). All of these language models - from encoders like BERT variants, to decoders like GPT-3 or encoder-decoders like T5 and BART - rely on hundreds of hours of pretraining, unsupervised methods to endow the model parameters with familiarity in natural language’s syntax and semantics. While incredibly useful, this task also encodes specific factual knowledge in the model that remains static over time, despite changing facts on the ground in the real world. Fine-tuning these models on a specific task or dataset is quite common but is a costly and non-localized method for changing a specific fact learned during pretraining. Performance continues to improve in increasing model size, so any believer in the scaling hypothesis should only expect model alteration to become a more pressing problem (at least in the near future). . With this in mind, I set out to explore alternative methods of knowledge editing - ways in which we could update a specific fact and its context without degrading a model’s performance on unrelated language samples. Concretely, imagine a setting in which a model is deployed to generate sentences based on political news stories that occurred each day. If that model was pretrained on a text dataset created 5 years ago, a prompt like “New policies announced by the President,…” would likely fail to produce the correct name. . With the guidance of Eric Mitchell, a CS PhD student in the Stanford AI Lab, I extended an existing framework for editing deep learning models to the space of autoregressive transformer decoders. . Editable Neural Networks . Selectively editing neural network outputs is not novel, though methods investigated vary widely in their approaches and implementation. One paper that seemed especially relevant was “Editable Neural Networks,” published by Sinitsin et al. as a conference paper at ICLR 2020. They view editing through an error-correction lens and propose a training approach that can reliably alter mistaken network output while minimizing overall disturbance and computational requirements relative to other approaches. With their technique termed “Editable Training,” the authors employ meta-learning to push model parameters towards specific objectives. The model should be fine-tuned on the specific base learning task at hand (eg. minimize cross entropy on a language dataset or classification error on ImageNet) while also learning to be adaptable when we change the gold label for a given example. Once the model has been trained with this procedure, it should be primed for quick edits to its outputs using just a few gradient steps. Sinitsin et al. explore image classification and neural machine translation use cases but do not touch on language model settings. . Meta-Learning: Learning to Learn . For many practitioners familiar with machine learning but without prior experience with meta-learning, it can be an unintuitive concept to grasp. In vanilla neural network training, an objective is specified by a loss function, the loss is evaluated over some training samples, and the parameters are updated by small steps in a direction that decreases this loss by some form of gradient descent. In meta-learning, there are multiple objectives and the model parameters are trained to be adaptable to several, sometimes competing, tasks. Instead of evaluating the training success over a concrete metric, we are endowing the model with the ability to learn faster in the future. . I specifically focused on a type of meta-learning known as MAML - model-agnostic meta-learning - introduced by Finn et al. in 2017. As they describe in the problem set-up: . The goal of few-shot meta-learning is to train a model that can quickly adapt to a new task using only a few data points and training iterations. To accomplish this, the model or learner is trained during a meta-learning phase on a set of tasks, such that the trained model can quickly adapt to new tasks using only a small number of examples or trials. In effect, the meta-learning problem treats entire tasks as training examples. . What does this look like in practice? There are some excellent tools available for MAML, and I found Facebook’s PyTorch add-on Higher to be quite easy to use. It allows us to grab the model parameters, compute intermediate gradients, and take some gradient steps functionally. Important to understand is the distinction between the “inner loop” and “outer loop” of MAML. I found the description provided by Zintgraf et al. in “Fast Context Adaptation via Meta-Learning” quite clear: . MAML is trained with an interleaved training procedure, comprised of inner loop and outer loop updates that operate on a batch of related tasks at each iteration. In the inner loop, MAML learns task-specific network parameters by performing one gradient step on a task-specific loss. Then, in the outer loop, the model parameters from before the inner loop update are updated to reduce the loss after the inner loop update on the individual tasks. Hence, MAML learns a model initialisation that can generalise to a new task after only a few gradient updates at test time. . In code, the training process looks like: . {python eval=FALSE} # init an “outer loop” optimizer for the total loss opt = torch.optim.Adam(model.parameters(), lr=1e-5) . # loop over data batches containing a base objective example and meta-learning task example for train_step, (base_example, meta_example) in enumerate(dataloader): # init an &quot;inner loop&quot; optimizer for meta-learning gradient steps inner_opt = torch.optim.SGD(model.parameters(), lr=1e-3) # higher takes in model and optimizer with higher.innerloop_ctx( model, inner_opt, copy_initial_weights=False, # by not copying weights, we directly alter the model parameters track_higher_grads=True ) as (fmodel, diffopt): #returns functional model and optimizer # specify number of gradient steps in meta-learning objective for _ in range(num_grad_steps): # calculate loss on meta-learning objective loss = fmodel(meta_example).loss # take an optimizer step diffopt.step(loss) edit_loss = fmodel(meta_example).loss # calculate loss on base objective base_loss = model(base_example).loss # backprop and optimizer step total_loss = base_loss + alpha * edit_loss total_loss.backward() opt.step() opt.zero_grad() . In the snippet above, you can get a sense of how MAML is implemented in practice. For more on MAML and meta-learning, I highly recommend this excellent blog post by Lillian Weng. . Experiments . I focused on a single toy setting - altering the names associated with a person in a given context. My approach incorporated several distinct steps to build and evaluate what I termed the “language editing” procedure. First, I constructed a novel dataset consisting of an unedited passage, a sentence of the passage with a permuted named entity, and a record of the named entity changed. Second, a MAML training architecture was written allowing for optimization over both a typical language model cross-entropy loss along with an “adaptability” loss. Finally, performance measures were created to understand how the language editing procedure altered model parameters and highlight areas for future improvement. . Data . The dataset was generated using WikiText-103, available on the Hugging Face datasets hub. I used a SpaCy named entity recognition model to collect all named persons and create pairs of original sentences and copies with swapped names. . Model . I used a pretrained DistilGPT2 model, an autoregressive transformer language model with fewer parameters than a typical GPT-2 model. This was chosen out of practicality, as MAML stores a copy of the model parameters in memory. . Objectives . Following the example set by Sinitsin et al., I evaluated three losses that were weighted and backpropagated through the network. In the inner-loop optimization, the model parameters were pushed to learn the edited named entity via a cross-entropy loss. In the outer-loop optimization, the edit loss is defined as the cross-entropy of the altered MAML model on the edited sentence. The base loss is the original model’s cross-entropy on the unedited passage. Finally a locality loss is imposed by taking the KL divergence between the probabilities output by the original and meta-learned model on the same passage. This attempts to preserve the meta-learned model’s performance on unedited samples. For more details, I suggest reading section 3 of the report. . . Results: Locality is Key . After several ablations, I found that this training procedure could allow a pretrained decoder to pretty effectively incorporate a simple edit with minimal change in overall perplexity on the validation set. This result was especially promising given the lack of success when the edit was applied to a simply fine-tuned model. . However, the result leads to obvious questions about the practicality of the experimental setting. I attempted to have the language model update on a single edit after the MAML procedure, but in a deployed model we likely would want to make hundreds of edits each month. I have been performing more research on varied editing settings and hope to find a robust methodology. . One lesson that I found to be key in this project is the importance of minimizing model degradation on other, non-edited samples. There were many hyperparameter settings that allowed for successful output editing, but many came with a high cost to the overall perplexity of the model. Looking at the actual language produced by these degraded models demonstrated that even small changes in parameters could render the models useless. . This all suggests that model editing is a rich area of research with many problems yet to be solved. I encourage you to check out the project report and code if interested! .",
            "url": "https://spencerbraun.github.io/blog/nlp/machine-learning/pytorch/python/meta-learning/projects/2021/07/14/selectively-editable-language-models.html",
            "relUrl": "/nlp/machine-learning/pytorch/python/meta-learning/projects/2021/07/14/selectively-editable-language-models.html",
            "date": " • Jul 14, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Unsupervised Text Style Transfer with Deep Learning",
            "content": "Natural Language Processing (NLP) is one area of deep learning that continues to make substantial, and often surprising, progress while also adding value to existing businesses. Supervised tasks like neural machine translation produce high quality, real-time results and Gmail’s predictive text feature often feels like magic. I have been most interested in recent applications of generative text models, such as using GPT-2 to play chess, write poetry, or create custom games. In that vein, I embarked on a deep learning project to see whether recent advances in style transfer, applying the style of one text to another while preserving the content, could be employed to increase the sophistication of a piece of writing. . Along with my coauthor Robert Schmidt, we looked into using adversarial autoencoders and transformer models to generate more sophisticated texts. Below I’ll outline our approach and conclusions, but our more detailed paper can be found here: Generative Text Style Transfer for Improved Language Sophistication . Autoencoders . After a wide-ranging literature review into unsupervised style transfer, we saw that autoencoders were the most common architecture employed and offered a diverse set of implementations. In its simplest form, an autoencoder consists of two parts: an encoder and a decoder. The encoder takes in a data matrix of a given size and produces its representation in a lower dimensional space. Concretely, its input layer might have 128 units while its output only has 64, forcing it to compress the information contained in the data. The decoder performs the opposite task, taking this lower dimensional, latent representation as input and outputting a reconstruction of the data. In a traditional implementation of an autoencoder, one might use a loss that penalizes differences between the original data and its reconstruction, thus encouraging the model to reproduce a copy of data from the compressed latent representation. . For the purposes of style transfer, there are added levels of complexity built into this basic model. While approaches differ across authors, the underlying idea is to separate the style space from the content space as part of the encoding, then train the decoder to faithfully recreate the content with a different style vector applied. In Shen et al., 2017, decoders are “cross-aligned,” meaning they attempt to align the generated sample from one style with the true sample from the other. In Vineet et al., 2018, the authors try to disentangle content and style latent spaces using a classifier to discriminate between styles adversarially. The encoder is trained to create a style-neutral content space by producing representations that leave the classifier unsure; the content is passed to the decoder with a style vector to produce a sentence with altered style. . While there were many such models to choose from, we followed the approach outlined in Zhao et al, 2018, an “adversarially regularized” autoencoder. This model is similar in that it employs a GAN structure to discriminate styles but employs a single loss function across encoder, decoder, and the style discriminator. . Transformers . While almost all of the papers in unsupervised style transfer published in 2017-2018 made use of autoencoders, we noticed that the newest preprints focused on transformer architectures. Transformers scrap the entire idea of content and style latent spaces - disentangling a sentence into these blocks is prone to error for subtle styles and fails to capture the complexity of the semantics in a limited vector representation. Instead transformers rely on a self-attention mechanism, a method of mapping dependencies among the words in a sentence rather than processing them sequentially. We rely on the Style Transformer proposed by Dai et al., 2019, in which style is seen as a distribution over a dataset. Similar to the adversarial autoencoder, a discriminator is used to attempt to categorize the style of a sentence. Then the content is preserved by passing a generated sentence through the network again, reversing the style transfer and attempting to recreate the original sentence. The sentence is nudged towards the target style by trying to fool the discriminator into assigning the target style as the most likely class. . Implementation . With some promising candidate models picked out, our work had just begun. One major challenge was finding datasets that would work well with these models - sophistication is hard to define and we did not simply want to transfer the style of a single author, such as Shakespeare. We ended up defining a “naive” dataset composed of relatively high scoring anonymized essays published by the Hewlett Foundation as part of a Kaggle competition on automated essay scoring. The “sophisticated” dataset was composed of diverse texts from Project Gutenberg and the Oxford Text Archive that had little dialogue or other features that might break from the author’s personal style. Texts were then stripped of common (“stop”) words as well as proper nouns using Spacy’s Named Entity Recognition (NER) API, though tags were changed to conform to the Stanford NER tags already used to anonymize the Hewlett data. Proper processing of the dataset helped produce more refined results and the authors and modifications were tuned throughout the process. . The embeddings, numerical vector representations of words and sentences, were also a key consideration in how the models operated. We could train embeddings ourselves or use pre-trained embeddings like GloVe. While we tried to use larger embeddings such as BERT, our GPU struggled with the memory required. . Finally, while we could read some of the output to get a sense of our success, we needed a more rigorous way of evaluating the output of these models. We made use of a few common scoring mechanisms, like BLEU and PINC, that calculate the similarity (and dissimilarity) between two sentences. This served as a crude measure of how much the model actually changed the words used in a sentence - too few and the model is mostly useless, too many and the content is likely not preserved. We then turned to the KenLM language model, which we trained on the sophisticated dataset, allowing us to measure the perplexity of style transferred sentences. Samples with low perplexity were more likely to come from the target language distribution, meaning they better reflected the sophisticated style. Finally we looked at some fluency scores like the Flesch Kincaid Grade Level and Flesch Reading Ease indexes that attempt to quantitatively estimate the reading level for a given sentence. . Conclusions . Most paper implementations of neural style transfer used sentiment as a proxy for style, and it is far easier for a discriminator to classify a sentence as positive or negative than pick up on sentence structure or formal arguments. While that might seem trivially true, the differences between naive and sophisticated texts were large and easily distinguishable for a human. Both the transformer and autoencoders are not yet prepared to distinguish more nuanced differences in language. . On the other hand, it was clear that the transformer was a real improvement over the older autoencoder models. Its sentences were more coherent and more clearly reflected some of the sophisticated style we were trying to capture. Given the novelty of many transformer models, it seems reasonable to expect continued progress towards capturing subtleties in language without simply scaling up the compute needed. . All of the code and processing used for this project can be found on Github. .",
            "url": "https://spencerbraun.github.io/blog/2020/06/08/generative_text_style_transfer.html",
            "relUrl": "/2020/06/08/generative_text_style_transfer.html",
            "date": " • Jun 8, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Lessons from Statistical Consulting",
            "content": "The Stanford Statistics department offers a unique service; any and all researchers from across the university can come discuss their ideas with Master’s and PhD students. Eager to see whether I could apply my coursework to actual questions of experimental design, inference, and prediction, I signed up to be a consultant for the quarter. The experience was as challenging as I had expected, but the difficulty did not stem from choosing the best model or interpreting p-values. In fact, most cases presented problems that were not actually “statistics questions,” but instead required a conversation about research goals and assumptions. . I spoke with students and professors working in medicine, biology, law, business, political science, and psychology, but they shared quite similar questions and concerns. Certainly some clients came in with difficult queries about experimental design, multi-level modeling, and valid inference (and had me quickly reaching for a textbook before attempting an answer). However, for most clients, I found that I could be much more helpful with a few probing questions instead of simply explaining how to fit a regression. Below I outline the most common misunderstandings and my efforts to work through them. .    . 3. The Streetlight Effect .   . The old joke goes that the drunkard looks for his missing keys under the streetlight, not because that is where he lost them, but because that’s where the light is. In a similar manner, some researchers start with a dataset and then try to formulate questions that might be answered using only this data. In principle, this may seem like a perfectly fine approach, but so often it leads to a tunnel vision in which one fails to ask whether the questions asked are interesting to the field or whether there are too many confounders and sources of variation outside this dataset to have any confidence in the findings. .   . For example, imagine you have found an exciting dataset published by the World Health Organization with some binary indicators describing various countries’ healthcare systems and their rates of heart disease. You might start down the path by thinking a regression could be a great model here, demonstrating how different features of a healthcare system affect actual health outcomes. Yet there are many problems to this approach. Health outcomes depend on much than the structure of a healthcare system, and by looking across the globe there is no reasonable assumption that you are looking at populations with similar diets, lifestyles, and access to medicines. By looking at a single time period, we cannot perform a causal analysis and may even measure a relationship that works in the opposite causal direction. .   . The found dataset may help inspire questions to research, but once those questions are formed the researcher should consider what information would be optimal and whether it may be reasonable to acquire it. They could consider looking for natural experiments to answer causal questions and those that might be most interesting to the subject matter experts they hope would read their work. In the end, these students didn’t need statistics help but instead needed to reconsider their process and design. .    . 2. Data Without a Plan .   . “Far better an approximate answer to the right question, which is often vague, than an exact answer to the wrong question, which can always be made precise.” . -John Tukey, The Future of Data Analysis .   . Thousands of protein levels in hundreds of cultures from 10 mice at 5 time periods. Five CT scans from 100 patients with four randomized treatments. Experiments are a lot of work; they often involve live animals or people and multiple measurements and time periods. If you embark on a study without defining a specific, data-oriented question you seek to answer as well as the statistical method you plan to use, there is a high chance you end up with a mess. .   . Let’s say you wish to measure variation across individuals. If you take many observations on a cellular level from 3 mice, you still only have 3 individuals. Your dataset looks large, but if you cannot assume those cell samples are independent from one another, you will have little to say about how variation affect individual outcomes. Alternatively, perhaps you collected the necessary data, but your study design has some features of a repeated measures ANOVA but also uses multi-level modelling and you want to make comparisons within and across levels and time periods. Structuring your study to fit into specific, well-tested models that rely on valid assumptions for your data will make your analysis much easier and your results more convincing. .   . It is easy to convince yourself that you have a plan. You have a general question that seems answerable with your current experiment design. Sit down at your computer and try to code your analysis with some simulated data. If you cannot translate your idea into code now, your study design might be in trouble. .    . 1. Asking Too Much of Statistics .   . On a good day, statistics can give an answer to the questions “how much?” and “with what certainty?” I say on a good day since we need the assumptions of any model to be at least somewhat valid to get these results, but for now let’s imagine they hold. As a researcher, your paramount objective should be how your work will fit into the current work in your field and how it will add to our understanding. .   . You may have a large number of datapoints, perfectly normal, homoskedastic errors, and your regression may have spot on estimates of predictor coefficients. But if what you really needed was a causal model, the statistics won’t help you here. Conversely, if you have a small number of datapoints that show an important effect in your field, don’t let your inability to get a 0.05 p-value stop you from writing a great paper and gathering support to collect more data. .   . In the end, statistics is a powerful tool for extrapolation, but it is up to the researcher to match the methods with a reasonable theory, careful research design, and orthoganlize their treatments. .",
            "url": "https://spencerbraun.github.io/blog/statistics/consulting/2020/03/16/lessons_from_statistical_consulting.html",
            "relUrl": "/statistics/consulting/2020/03/16/lessons_from_statistical_consulting.html",
            "date": " • Mar 16, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://spencerbraun.github.io/blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Yield Curves in Dash",
            "content": "With a lot of recent focus on the direction of US interest rates and the likelihood of a recession, I thought I would play around with Treasury yield data to construct an easy way to keep an eye on how yield curves are changing. For those unfamiliar, yield curves are simple plots of the yield of a Treasury bill, note, or bond versus the maturities of the securities. The Treasury publishes the market yield for each maturity daily. . Using Dash and Plotly, I created a simple tool to pull in the daily yield data and create some metrics. Feel free to clone from github here. Below I’ll explain how to use and interpret the metrics displayed. . Historical Yields . The first chart simply shows the historical path for yields for each Treasury maturity. Lately while shorter term maturities (say &lt; 1 year) have been relatively flat, longer term maturities have sharply declining yields. The Federal Reserve has more direct control over short-term maturities by setting the target range for the Fed Funds rate and providing guidance through commentary and dot plots. Longer term yields are a reflection of the market’s expectation of future interest rates. As economic data has weakened and the Fed has turned from raising rates to a potential easing cycle, longer term Treasury yields have declined. . . Yield Spreads . Yield spreads are one place to look when discussions of an yield curve inversions are omnipresent. An inversion occurs when a shorter term Treasury becomes higher yielding than a longer term maturity. What maturity pair determines when the curve is officially inverted? Market participants do not fully agree on this question, so the chart below shows the spreads between yields of three different maturity pairs: 10 years / 2 years, 10 years / 6 months, and 5 years / 2 years. When the spread is below zero, the longer maturity Treasury has a lower yield than the shorter one, meaning that yield spread in inverted. So far the 10yr-2yr spread has not inverted, while the other two have bounced in and out of inversion. . . Typically, one might expect long term Treasuries to yield more than shorter term due to a term premium and the expected path of future interest rates. When investors expect the economy to continue to grow steadily, eventually they would expect the Fed to increase interest rates to cool the economy before it overheats. Additionally, investors carry duration risk with longer-term treasuries since they do not know the future path of rates, which typically would command some premium in the yield of an asset. When the yield curve inverts, the market is sending some of these potential signals: - The market expects future interest rates to be below current interest rates due to an economic contraction. The Fed will cut rates in response to deflationary pressures or increasing unemployment, making future interest rates lower than current interest rates. - A coming recession will send investors running from risk assets. Treasuries are the most popular risk-free asset around, so investors will pay more for longer-term Treasuries as a way to protect their principal. More demand pushes up the price of those Treasuries, decreasing their yields. . Yield Curves . Finally, the dashboard has a graph of yield curves - every maturity plotted against their yields on a specific date. By plotting the trailing 5 days, we can see how the shape of the yield curve has changed from the recent past. Trade tensions and the recent reduction in the target Fed Funds has sent yields plunging in the last week. . . Feel free to reach out with comments, questions, or suggestions. .",
            "url": "https://spencerbraun.github.io/blog/economics/finance/python/projects/2019/08/07/tradedash.html",
            "relUrl": "/economics/finance/python/projects/2019/08/07/tradedash.html",
            "date": " • Aug 7, 2019"
        }
        
    
  
    
        ,"post7": {
            "title": "Book: Doing Capitalism in the Innovation Economy",
            "content": "Doing Capitalism in the Innovation Economy by William Janeway . Bill Janeway has a lot to say in this book, and he does so with dense and clear prose. In three points, this book makes clear: . The innovation economy is a three player game among the state, the market economy, and financial institutions. The game is played with irreducible uncertainty, and playing by the rules of homo-economicus (some neoclassical rational agent) is not a successful strategy. | Speculation and state funding are essential to pushing out the knowledge frontier - waste and tumult should be expected and are needed to fund ideas that may not be cash positive in the near future. | Do not expect this process to be orderly; the most valuable innovations accompany a large amount “Schumpeterian waste,” the money and time plowed into speculation and dead-end investments. Schumpeter’s theory of creative destruction in which waste and excess play a prime role in disrupting entrenched players is key. | . While he covers a wide variety of topics in detail, Janeway spends a great deal of time showing how these themes are viewed through the lens of investment strategy, bubbles, and the State’s role in the economy. . Investing . The architecture of investing has been uprooted over the last few decades - regulatory changes, decreases in fixed brokerage commissions, institutionalization of savings. Each change has further pushed firms to act as principals instead of agents, and the shift in incentives and compensation generation had a direct impact on the financial system. Securitization allowed firms to originate and transfer assets for huge profit while increasing dependence on both funding liquidity and market liquidity. Any new business entering this financial arena should have clear eyes about where incentives align and diverge. . For business management, “Cash and Control” are the essence of liberation from the constraints of competitive markets and economic efficiency. Cash buys time to figure out what is going on; Control permits the player to use that time to shift the parameters of the problem. The biggest winners in the innovation economy choose no financial risk at all. While winners take all shapes and sizes, there is a consistent formula that produces results: demonstrate competence to gain trust, leverage the trust for capital and control, redound on demonstrating competence in new role, repeat. Janeway points to JP Morgan as an example of this cycle, but countless examples can be found. . For an investor looking at a business, there is no notion of “right” valuation. Discounted cash flow, technical market metrics, and potential sale price all are dependent on judgment; predicting the future is embedded in the number produced. When there is a widespread consensus, we see steep discounts or premia, but uncertainty has not disappeared. Financial markets exist to intermediate the uncertainty, expressed through differences in opinions and valuations. Aligned incentives and trust leads to access to capital, the gate that gives investors the ability to pounce when prices are in their favor. The investor is not pitching a specific investment to the capital holders but pitching their own judgment to access capital freely and deploy it strategically. . How do businesses in the innovation economy differ from others? Janeway offers a number of useful mental models used to segment companies and businesses: . Projects vs. Products: Projects are owned by the customer, have an expiration date, and are custom built for each client. A product requires an ongoing relationship between customer and company, customizable but not custom for each client. | State pipeline: There is a network connecting state funded science, research commodified into product, and private demand for this product. Biotech has been a focus of VC following this model - state funded pipeline of research ready to be deployed into treatments, demand is funded by insurers and is inelastic, TAMs are known from the outset, and risks beyond scientific are estimable. | Innovator’s Dilemma: an original innovator now with surplus resources is crippled by its own inability to cannibalize its margins with innovations that will eventually take them anyway. Janeway focuses on the early computer industry, where disruptions were plentiful and big players like IBM stumbled to adjust to open source protocols. This point seems tied to commoditizing your complement; finding the points of leverage (or Power in Helmer’s terminology) in complementary components to your product and turning them into interchangeable parts. | UI vs Tech: The best technology does not always win, and often fails to win. Customers do not see or care about the best technology, but want the best product. Often the founders with the best technology dismiss the importance of user-interfaces, marketing, and sales. | The Paradox of Prudence: In tough times, companies shrink their balance sheets and households tilt portfolios away from real investment towards safe assets. This is micro-prudent but macro-imprudent as it makes the economy more risky as liquidity retreats. Attempts to reduce individual risks raises endogenous risks. Like Keynes’ Paradox of Thrift - individual increases in savings can reduce aggregate savings by reducing economic activity. | . Fun Detail . Venture capitalist Fred Adler adds some levity to the narrative and has some choice quotes. “There is no such thing as a fixed cost.” Every cost is variable in the long run, and the only question is simply how much money and time it takes to turn a fixed into a variable cost. | The definition of corporate happiness: “Positive cash flows from operations.” This fits neatly with the broader idea of Cash and Control; without positive cash flow, you do not control your own destiny. | . | . Bubbles . Janeway’s dual role of investor and economist shines in this chapter, as he connects economic models to his lived experience. Key to this chapter is his use of the Rational Expectations Hypothesis (REH), the idea that financial markets behave as if they were complete and efficient through the mind of a rational representative agent, one who knows all relevant information. Add to this model the Efficient Markets Hypothesis, and the market will efficiently allocate capital to the best ventures. With these two ideas, we can construct the idea of the rational bubble: a continuous rise in the asset’s price leads investors to be content to hold at the new valuation, as the risk for the bubble bursting is offset by potential rate of price increase. Bubbles can have rational behavior at an individual level while still exhibiting a full coordination failure of the market. On the other hand momentum driven behavior, increased buying as prices rise and increased selling as prices fall, goes against what we might expect under REH. . Market bubbles are often accompanied by moralizing or the assumption that an investor should position defensively. Janeway has no qualms demonstrating how an investor can take advantage of the upside as well, and shows how Warburg Pincus benefitted from price bubbles in their holdings. By recognizing the bubble, the firm was able to divest and distribute towards the peak of valuations. However, one must not cloud personal judgment by substituting the market’s enthusiasm for one’s own projections. . Bubbles certainly have their downsides. Financial speculation can have a direct negative role on the innovation economy, discrediting innovative technology and retarding its progress as it struggles to find funding after the crash. As Hyman Minsky noted, the Fed steps in to prevent catastrophe and maintain markets, validating the past use of a risky instrument and serving as a guarantee on future speculation, inevitably requiring another intervention down the road. Minsky’s economic model, long overlooked before gaining notoriety in the aftermath of the GFC, posits that the economy naturally swings between stable and unstable financing regimes. Over periods of prolonged prosperity, the economic momentum alters financial relations towards levels of indebtedness that threaten the real economy. The credit system rides waves through stages of confidence and risk taking from hedge finance to speculative finance to Ponzi finance. . But just as often, bubbles are essential to the pushing the boundaries of the innovation economy. Speculative investments that fail in the short-term provide a path towards success in the long-term. Productive bubbles differ from the purely financial ones; the bubble in US railways in the late 19th century funded massive infrastructure investment with tracks laid across the country. The crash may wipe out equity holders, but the real assets remain to be utilized by the next generation of companies. . What separates good bubbles from bad? There is a distinction between the object of speculation and the locus of speculative activity; the object defines the assets that inflate in value while the locus defines how widespread the exuberance extends. In other words, is the speculation confined to the capital markets or does it suck in depositors, credit extension, and systemically important institutions. The degree of leverage can determine whether a bubble is productive or destructive, since following Minsky’s model large defaults and restructuring make the locus of losses likely to spread beyond the capital markets. We can view 2000 as a productivity enhancing bubble confined to the equity markets and 2008 as a credit driven bubble spread throughout the banking system with negative effects for the real economy. . The State and Macroeconomics . Here the economist Janeway is in full force, and many theories of state power and the role of capitalism are contrasted. The government plays a key roll in fundamental economic processes of growth, like promoting . Keynesian waste: deadweight loss represented by unemployed resources of human labor and physical capital. Keynes worries economic inactivity has long-term effects on productivity as skills atrophy and capital depreciates. It is preferable to employ workers in completely wasteful engagements, as maintaining zero productivity growth is better than a permanent loss of output potential. | General purpose technology: widely used, capable of ongoing improvement, enables innovation in application sectors. Basic scientific innovations build thick stacks of technologies and applications. | Spillovers: technology spillovers increase the productivity of other firms that operate in similar areas. | . While the specific points outlined are interesting on their own, we may still ask what kind of state generates productive investment? Janeway sees several key roles for the state. First, strong patent policies, along with protective tariff policies for fledgling economies, provide the bedrock for private investment and enterprise. Second, the state is there to support fundamental research into new technology, though this point is not without political controversy. Janeway points to the left, which often sees new technologies as job destroying. . As a US example, world wars supplanted private investment in research with defense-led initiatives and changed corporate patent structure by requiring major corporations to share results of research with each other and new entrants. Here the state fronted the cost of knowledge generation and made sure society benefitted through diverse commercial applications of the science. In contrast, much of Europe followed a model of protection for “national champion” companies, with quite apparent differences in results. . Finally, there is good point raised about the limits on the value of economic models. The macro economy is not something that any of our models easily predict, summarized as the “database problem.” Econometric modes are reliant on historical data to predict future moves, yet if we are living outside of the support of the data (eg. behavioral relationships are non-stationary) then our predictions may have no value in the current environment. This is where simulation is key; try to model agent-based incentives which should be stationary and simulate responses given possible states of future reality. This idea is reminiscent of Thomas Schelling’s Micromotives and Macrobehavior, which shows how structural problems can be understood through individual incentives. . Second, not only can we fail at estimating the parameters in our model, but we miss the complex structural relationships that can lead us to false conclusions. Ricardian equivalence, a model in which state borrowing is economically equivalent to state taxation since all actors know borrowings must be returned out of future tax revenues, is a fine example of over-reliance on narrow minded rational-agent models. This idea suffers from the unlikely assumption that all potential resources are already employed, so state spending comes without a multiplier. Keynes instead posited that state spending promotes the use of unused assets, boosting economic activity and potential state revenues at the same tax rates. . Conclusions . Doing Capitalism is a nice complement to M. Mitchell Waldrop’s The Dream Machine, turning a vivid example of government backed science into a fully developed economic theory. While State money kicked off the internet revolution, the privitization of innovation described in the The Dream Machine has only grown. The US government shows less interest in funding scientific corporate initiatives, and political incentives aren’t aligned toward producing the type of waste Janeway prizes (think Solyndra). However, private development has also taken a turn in the machine learning age, with the largest research companies offering their algorithms via API for free. . While popular business models have evolved (positive cash flow from operations is not exactly in vogue), Janeway provides a valuable mental model for understanding our modern economy and how the next innovation jump might arise. The exuberance of the crypto bubble left many with nothing to show for their investments, but it also introduced new funding mechanisms for distributed teams, incentivized developers to learn the new language of smart contracts, and opened experimentation in trustless systems. It’s too soon to declare the craze a productive bubble, but at the very least the crash failed to spillover into the real economy. The State also shows some interest in returning to the funding of science with an RFQ for tech that requires true advances. As a buyer of private space services and with the spectre of green investment in the future, the federal government could once again act as a scaffold to push the innovation frontier out further. .",
            "url": "https://spencerbraun.github.io/blog/economics/investing/books/2019/07/03/doing_capitalism.html",
            "relUrl": "/economics/investing/books/2019/07/03/doing_capitalism.html",
            "date": " • Jul 3, 2019"
        }
        
    
  
    
        ,"post8": {
            "title": "Book: The Dream Machine",
            "content": "The Dream Machine by M. Mitchell Waldrop . In The Dream Machine, M. Mitchell Waldrop loosely tracks the career of J.C.R. Licklider, an MIT researcher and ARPA administrator who had a grand vision of human-computer symbiosis and networking. . More generally, Waldrop narrates the story of the modern computer, which one might simply see as a linear progression. Starting with WWII, mathematicians devised physical systems that could compute answers to specific questions, such as a mechanical differential analyzer. Over time, those physical systems were reproduced electronically, with large computers with vacuum tubes hooked up to mimic specific equations. General architectures were designed and suddenly batch processing could be done on general machines. Time-sharing allowed interactive computing to arise as a central computer could respond to many inputs in what appeared to be real time. This led to the development of personal machines and with it, the network that connected them all. . The value of Waldrop’s account comes from his ability to detail how ideas were formed and innovations produced. What seems like an obvious transition from centralized to personalized computing was full of debates, path dependencies, great teams, and some visionaries pushing work forward. It also ties together the role of public and private partnership in developing new science, providing a base layer on which the rest of technologies and applications are built. Below are some resonant points from the account: . Diverse Teams and Expertise . Many of the key innovators in the book were working on conventional ideas until a chance encounter with someone working on related architecture. Isolation within a field inhibits innovation, whereas surrounding yourself with the creative, challenging assumptions and predetermined paths, and mixing with people both alike and unalike can yield results. . Much like the thesis behind David Epstein’s Range, Waldrop makes clear that many innovations in computing were not pulled out of thin air but required adaptations of prior results and collaborations with other fields. Shannon’s information theory equations were inspired by entropy in physics, yet he re-derived them himself without consulting texts. While his reengineering from first principles alone is impressive, one can cut corners by seeing how other fields use models and mathematics. The basic building blocks we consider definitional to computers - mice, WYSIWYG, text editing - were developed in the field of Human-Computer Interaction, requiring the fusion of computer science and psychology. . Additionally, no single person pushed the computer or the psychological models forward alone. George Miller collaborated with Noam Chomsky to produce the field of cognitive science, applying structural paradigms and “states” to the mind. Every player learned at conferences and read the ideas of fellow researchers. Certainly there are counterexamples, and Shannon deserves a lot of credit, but his work alone could not produce the modern machine. . Building Blocks of Innovation . Government money produced this revolution. Ample liquidity without constraints is necessary to allow crazy ideas to be tried, and ultimately a lot of those investments will end in failure. While the investments had a purpose, they did not center around an expected return. ARPA and its offshoots paid millions for duplicative and early stage technology that would be replaced within a decade. . Chance plays a large role in Waldrop’s telling, and even Silicon Valley’s dominance in computing seems to stem from a mix of short-sightedness from MIT and Harvard and a greater willingness to experiment in newer institutions. Few of the older players were excited by networking and time-sharing, and in failing to push for innovation, they missed the next big thing. Waldrop traces the rise of the West to the first Arpanet connections that started in UCLA, which drew the best networking engineers to California. . Eventually the innovation engine runs dry. It must. As ARPA invested in technologies, they became commoditized into products, drawing private contractors into the field. Large companies bankrolled computer scientists to perform proprietary research. As the public good became a private one, collaboration grew more difficult, drawing more and more scientists into the private sector to access the cutting edge. The state funded a base layer of pure science with knowledge spread widely; industry found a way to make it profitable and useful. . Careers and Management . Careers are not made overnight. The creators of the information revolution had both large successes and failures. It was their passion for the material and focus on research that eventually yielded success - chasing success for its own sake is unnecessary and can be myopic . One of the most compelling portions of the book details the rise and fall of Xerox PARC. While the fall is well-known, I was more interested in how Bob Taylor created an unparalleled innovation lab that produced Ethernet, laser printers, modern text-editors, and many more computing paradigms. A few takeaways on his management style: . Turning class-one arguments, which were screaming matches, into class-two, in which each side had to explain the other side’s position to the other side’s satisfaction. Behind each argument lurked unspoken assumptions and facts that only one side was aware of, and only by proving you understand the other side can your position be considered in good faith. | Presenting the work of one group or individual across departments and having a rigorous question and answer session. Pushes greater collaboration, inspiration, and is a great reality check for an idea with an unfamiliar audience with different expertise. | Hiring by committee - decisions were not made at the top but were voted on as a department. Hiring because people are smart not because they know the exact thing a group has been trying to solve. | .",
            "url": "https://spencerbraun.github.io/blog/books/economics/2019/01/06/the_dream_machine.html",
            "relUrl": "/books/economics/2019/01/06/the_dream_machine.html",
            "date": " • Jan 6, 2019"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I’m a graduate student at Stanford, working towards a Master’s in Statistics. My coursework is largely in prediction and machine learning, employing statistical models like regularized linear models and basis expansions as well as deep learning techniques. I have worked on projects in generative language modeling, recommendation systems, experimental design and implementation, tabular data augmentation, and document embeddings. Most recently, I worked at C3.ai for the summer using data science techniques to improve COVID-19 modeling and create publicly available data resources. .   . Before returning to school, I worked in New York at Hudson River Trading, a high-frequency algorithmic trading firm. There I served as a Python developer, automating clearing and settlement, conducting statistical analyses, and coordinating technical development for new initiatives. Prior to HRT, I spent some time in DC working for Berkeley Research Group constructing damages models on behalf of firms in litigation. .   . I’m a self-taught programmer and spend my free time working on personal projects and learning new frameworks. I’m a fan of economics blogs, Fed watching, and Russian lit. . Work . Summer 2020: C3.ai, enterprise AI software provider . 2016-2019: Hudson River Trading, high-frequency algorithmic trading firm . 2014-2016: Berkeley Research Group, expert-driven consulting firm. . Contact Me . Email LinkedIn Github Twitter .",
          "url": "https://spencerbraun.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://spencerbraun.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}