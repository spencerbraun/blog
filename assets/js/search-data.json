{
  
    
        "post0": {
            "title": "Demand Forecasting with Bayesian Methods",
            "content": "The Stanford Medical system sees a constant stream of patients through its doors every year, many of whom require intensive care. Blood transfusions are a common occurrence as part of a therapy or surgery, but blood platelets are a scarce resource that have a short shelf life. Therefore accurately predicting future demand is essential in order to minimize waste while ensuring hospitals have enough on hand for the current patient population. In 2017, Guan et al. published an initial model employing a frequentist approach, minimizing waste in a linear program with constraints that took patient features and current stores in account. This model was eventually rolled out into production in an open source R package titled SBCpip and currently serves the medical system. . I investigated two alternative Bayesian methods to predict demand more transparently: online regression and Gaussian processes regression. Bayesian models offer a number of key advantages over the current system. First, while the linear program approach outputs simple point estimates for the optimal inventory to have on hand, these Bayesian procedures generate a richer posterior distribution that can be used to quantify uncertainty and quantify how the model inputs impact predictions. Second, the data is messy and manually extracted into excel files, leading to duplicates and nonsensical values. The methods explored offer a higher degree of robustness to outliers, which can be tuned by varying the prior distributions. Third, since other hospital systems have similar blood platelet demands, there have been some efforts to generalize SBCpip to other facilities. The Bayesian approaches considered would allow models to lean more heavily on priors before adequate data has been generated and become more customized to each facility over time as data accumulates. Fourth, the linear program approach predicts demand indirectly, making results much harder to interpret. The methods proposed here allow for greater flexibility and transparency in diagnosing issues and determining the underlying drivers of a particular prediction. . Approaches . While the data cannot be made publicly available, there are a few key features to note. Our outcome variable, blood platelet units used, helps determine the best types of models to use for this problem. The amount used is a real valued number that closely follows a Gaussian curve, permitting the use of a rich class of normally distributed models. The regular fluctuations suggest that some seasonality may be present that could be picked up by a Gaussian process regression model given a suitable kernel. Below I consider some plausible Bayesian models that each have tradeoffs in simplicity, accuracy, and utility. . Online Regression . In a typical Bayesian regression problem, we have a setting with independent and identically distributed training data with future observations drawn from the same population. In this case we can simply place a prior over the weights and variance, evaluate the data likelihood and find the posterior through exact or approximate inference to determine a stable model from which we can make predictions. Here we are presented with a time series of features and inventory data and want our model to flexibly model trends that depend on our time step. Online regression is a naive baseline for this problem in which at time step t, we take the posterior from time step t − 1 as our prior and update the model using only the likelihood over the most recent batch of data. Define our original priors as $p( theta_0)$, then our model updates occur sequentially: . begin{align} p( theta_{t}) = p( theta_{t} mid y_{t-1}, x_{t-1}, theta_{t-1}) &amp; propto p( y_{t-1} mid x_{t-1}, theta_{t-1})p( theta_{t-1}) p( theta_{t+1} mid y_{t}, x_{t}, theta_{t}) &amp; propto p( y_{t} mid x_{t}, theta_{t})p( theta_{t}) end{align} . While a simple regression may seem underpowered for this task, it provides an excellent baseline against which we can compare more complex approaches. The conjugacy of the Gaussian model makes it extremely efficient as we add new data points with easy interpretation. However, its failure to model dependency between data points may introduce bias that could be avoided with more complex approaches. . The model as implemented closely follows the approach outlined in Christopher Bishop’s Pattern Recognition and Machine Learning. Given model $y_i = w_ix_i^T + varepsilon_i$, we give likelihood and prior distributions as $p left(y_{i} mid x_{i}, w_{i} right)= mathcal{N} left(w_{i} x_{i}^{ top}, beta^{-1} right)$ and $p left(w_{0} right)= mathcal{N} left(m_{0}, S_{0} right)$ respectively, where $ beta$ is a precision hyperparameter, $m_0 = mathbf{0}$ and $S_0 = diag( alpha^{-1})$ for hyperparameter $ alpha$. Our posterior is analytically derived as begin{align} p left(w_{i+1} mid w_{i}, x_{i}, y_{i} right)= mathcal{N} left(m_{i+1}, S_{i+1} right) S_{i+1}= left(S_{i}^{-1}+ beta x_{i}^{ top} x_{i} right)^{-1} m_{i+1}=S_{i+1} left(S_{i}^{-1} m_{i}+ beta x_{i} y_{i} right) end{align} . For model criticism, Bishop recommends evaluating the marginal likelihood, or model evidence, which he derives analytically. For M features and N observations, the log marginal likelihood can be expressed as begin{align} log p({y} mid alpha, beta)= frac{M}{2} log alpha+ frac{N}{2} log beta-E left({m}_{N} right)- frac{1}{2} log left|{S}_{N}^{-1} right|- frac{N}{2} log 2 pi end{align} Therefore we can directly use this expression to select for optimal hyperparameters $ alpha, beta$ to improve the model fit. . Gaussian Processes Regression . I look to Gaussian processes (GP) regression to correct for the deficiencies of the regression model. While the prior model considered datapoints to be independently drawn from a stationary distribution, Gaussian processes explicitly model the covariance relations between time steps. Here we consider our feature vectors $x in R^d$ over T time steps $x_1,…,x_T$ and each observation of inventory used $y_i = f(x_i)$. If we assume the functions $f$ to be given by a prior Gaussian process $f sim GP( mu, K)$ for some mean $ mu$ and covariance matrix $K$ and $y_i sim N(f(x_i), sigma^2)$ then we obtain posterior $p left({f} mid x_{n}, y_{n} right) propto {N} left({f} mid { mu}^{ prime}, {K}^{ prime} right)$ for ${K}^{ prime}= left({K}^{-1}+ sigma^{-2} {I} right)^{-1}, ; { mu}^{ prime}={K}^{ prime} left({K}^{-1} { mu}+ sigma^{-2} {y} right)$. Similarly, we can obtain an exact posterior predictive distribution given by begin{align} y_{T+1} | x_{T+1}, x_i, y_i &amp; sim N(m_{T+1}, v_{T+1}) m_{N+1} &amp;= mu left({x}_{N+1} right)+{k}^{ top} left({K}+ sigma^{2} {I} right)^{-1}({y}-{ mu}) v_{N+1} &amp;=K left({x}_{N+1}, {x}_{N+1} right)-{k}^{ top} left({K}+ sigma^{2} {I} right)^{-1} {k}+ sigma^{2} end{align} . GP regression offers additional modeling decisions that improve its attractiveness for our use case. First, there are a variety of potential kernels for expressing the relationship between feature vectors over time. Our choice determines how closely we model regular periodicity and linear trends in the data. . Additionally, since GP regression is closely related to time series methods like ARIMA, we can eschew using features at all and treat the blood inventory as a standalone data series. Why might we prefer this approach? The other regression approaches obtain a posterior predictive distribution by conditioning on $x_{N+i}$ for $i in [1,7]$, feature vectors which are supposed to contain data like patient counts and other measures that aren’t actually available for future days. While features could be extrapolated from current data, a GP regression approach can condition simply on the time step and avoid some of these messier modeling decisions. . Experiments . Over the experiments run, the simple online regression obtained the lowest mean absolute error across 1, 3, and 7 day prediction intervals. This result is somewhat surprising given the difference between the modeling assumptions (eg. independence) and the true data generating process but can be reconciled when we consider that we limited GP regression’s performance by limiting the features used in fitting. While online regression achieved the best results under the evaluation metric, GP regression may still be preferable in a practical setting. Comparing the mean absolute error among the methods explored: . begin{array}{l|cccc} Model &amp; MAE , 1 , Day &amp; MAE , 3 ,Day &amp; MAE , 7 ,Day Online Regression &amp; 6.40 &amp; 6.65 &amp; 6.58 GP Regression &amp; 7.33 &amp; 7.80 &amp; 7.89 end{array} . Online Regression . The regressions were tested under two scenarios: for each time period, we could either fit the model using data since inception or look back over a fixed window. The data since inception approach was too constraining and performed slightly worse in evaluations, leading the fixed window approach to be used for comparisons. The model was fit over 7 day increments and produced predictions for 7 days into the future. . We can understand the performance of the model by looking at the results more closely. First, while the online regression approach does not explicitly model a time series, one-hot encoded day of the week features are included as input to the model. Therefore it still can capture some of the cyclicality of blood platelet demand. . The online regression was fit over a grid of potential hyperparameter values for $ alpha, beta$, with the model with the highest marginal likelihood selected for final predictions. It was sensitive to extreme values of each hyperparameter, but otherwise relatively robust to misspecification within a neighborhood of values. . Gaussian Processes Regression . GP regression offers a variety of modeling decisions, but perhaps the most important is the choice of kernel. This decision directly impacts the model’s ability to reflect the true relationship between observations. In “Automatic model construction with Gaussian processes” David Duvenaud outlines a decision process for choosing the best kernel given a dataset. I experimented with permutations of additive kernels, combining Exponential Quadratic, Linear, Periodic, and Locally Periodic varieties. The Locally Periodic kernel was of particular interest as the data shows consistent day of the week seasonality but with magnitudes that vary over time. . The model was fit in TensorFlow Probability by maximizing the marginal likelihood of the model parameters and the observation noise variance over the training data via gradient descent. The GP regression model was then fit on the data using the optimized kernel before posterior samples were drawn. Over all kernel experiments, a combination kernel of Exponential Quadratic and Locally Periodic varieties produces the best results. . . The model was then fit for periods increasing in 7 day increments from the data series inception, with predictions made for the following 7 day increment. For example at day 100, data from days 0-100 are taken as the training set, with the mean and standard deviation from the posterior predictive distribution forming confidence intervals for the prediction period. Alternatively, samples could be drawn from the posterior predictive distribution to simulate different paths created by the Gaussian process. . . We can make further sense of GP regression’s underperformance by looking at the kernels learned themselves. Comparing kernels fit in the training process along with the true observations, we can see that given little data, the kernel is unable to adapt to the variation in the data. A kernel with parameters trained on the full dataset shows much more reasonable variation, with both global and local trends. However, comparing this kernel to the observed variation, we can see it still has a long way to go to match the ground truth. Additional data would go a long way to improving this method, but there may also be more expressive kernels than those explored that could drastically improve performance. . While the hyperparameter search was partially limited by compute time, GP regression’s results indicate the method could show promise in this application. While it achieved higher error than other methods, its performance could improve if we included additional features, some of which might be known ahead of time by hospital administrators. Additional variance control, such as full marginalization of the model’s hyperparameters, would also increase its effectiveness. . Conclusions . I took a survey approach to understanding the tradeoffs among Bayesian methods for resolving a difficult, real-world problem. Forecasting in general is a daunting task, since it requires extending a model beyond the domain of observations seen. In the medical setting, it is even more crucial to have humans in the loop, and while the original linear program could set hard boundaries on the predictions produced, it is more useful to ensure that end users have an intuitive understanding of the results produced and the tools to probe alternative outcomes. . The Bayesian methods explored each had significant problems in completely satisfying the objectives set out in the introduction, but the approaches could be considered a starting point for additional refinement. In particular, GP regression’s ability to generate sample trajectories and credible intervals could prove valuable for those in charge of inventory ordering decisions. Quantified uncertainty can be vastly superior to relying on heuristics. . Finally, the experiments made clear that the data may not contain sufficient information for more exact predictions. Some effort should be expended to encourage the medical system to maintain more expansive records, enabling models to find the best features that drive blood platelet demand. .",
            "url": "https://spencerbraun.github.io/blog/bayesian/machine-learning/python/statistics/projects/tensorflow/2021/07/22/demand-forecasting-with-bayesian-methods.html",
            "relUrl": "/bayesian/machine-learning/python/statistics/projects/tensorflow/2021/07/22/demand-forecasting-with-bayesian-methods.html",
            "date": " • Jul 22, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Chinese Word Segmentation: Classic and Modern Methods",
            "content": "Part of the fun of NLP is the diversity of language structures that models must consider. Tools that are built in a monolingual context can easily fail to achieve more global utility. If you just work in English, you may find word embeddings fun and useful, but the German Komposita don’t fit neatly into our English-centric boxes. Similarly, once we leave the Latin alphabet, we acquire all sorts of new challenges. . Chinese sentences do not typically contain spaces to separate words. From context it may be clear which characters combine to form distinct words, and finding ways to endow this ability to a model is a classic NLP task. I looked to see how well classic machine learning algorithms could learn to accurately segment words and then explored some more modern methods that can also offer a richer set of labels. . Binary Classification . In “A realistic and robust model for Chinese word segmentation”, Chu-Ren Huang and co-authors treat segmentation as a binary classification task, predicting whether a word separation exists between two characters or not. This approach has obvious appeal, as it reduces a seemingly complex problem into a simple task. . They examine the efficacy of a number of classifiers, including LDA, logistic regression, SVMs, and feed-forward neural nets. While there are some performance differences among them, the real work involved stems from engineering the features that are input into these models. This is the common refrain for modeling language before deep learning subsumed the field and could easily learn in an end-to-end fashion. . First, we must find adjacent characters for the model to classify, but we likely also want some context since just two characters may not be enough information even for a native speaker. Huang et al. construct 5 features per sample such that for a character string like [ABCD] we end up with [AB, B, BC, C, CD]. For maximal data efficiency, we want this for every pair of adjacent characters in the training set. I’m working with a dataset with spaces inserted into sentences to show the true word breaks, so my task is to both featurize and label this dataset. I constructed the samples by passing a sliding 4-character window over each sentence: {python, eval=F} def create_samples(sentence: str) -&gt; str: “”” Breaks passed sentence into all combinations of 4 adjacent characters using a sliding window over the sentence. Creates label 1 if word break exists in the middle of 4 characters, 0 if not. “”” . split_line = sentence.strip().split(&quot; &quot;) line_breaks = [len(x) for x in split_line] cum_breaks = [sum(line_breaks[0:x]) for x in range(1, len(line_breaks))] offset = 0 samples = [] for idx in range(len(split_line) - 1): curr_len = len(split_line[idx]) string = &quot;&quot;.join(split_line)[offset : offset + curr_len + 3] if len(string) &lt; 4: break for pos in range(curr_len): sample = string[pos : pos + 4] if len(sample) &lt; 4: break label = 1 if (offset + pos + 2) in cum_breaks else 0 samples.append((sample, label)) offset += curr_len return samples . This system captured all consecutive strings in a sentence, producing 2,638,057 strings with a word break in the center and 1,591,532 strings without a center word break in the training set. While this dataset is somewhat imbalanced, the test set had a very similar balance post-processing. I then split each sample into the 5-feature format {python, eval=F} def featurize(sample: Tuple[List[str], int]) -&gt; tuple: “”” Given a sample tuple of a 4 character chinese string and a label, (‘ABCD’, 1), returns featurized version of the form ((‘AB’, ‘B’, ‘BC’, ‘C’, ‘CD’), 1) “”” chars, label = sample features = (chars[:2], chars[1], chars[1:3], chars[2], chars[2:]) . return features, label . Now we have distinct samples and features, but we still aren’t ready for modeling. An SVM doesn’t place a hyperplane in the space of Chinese characters - we need numbers. In the deep learning space, we would be ready to assign dense word and position embeddings, but here we will turn to one-hot vectors. To construct we follow the steps: . Each character is assigned an integer index | For each feature string in a sample, we take a zero vector of dimension equal to the size of the vocabulary and fill in 1 in the index corresponding to the characters in the string. | Concatenate all five vectors from each feature to form a very long, sparse vector for each sample | . I played with some additional changes, such as separate indexing of unigrams and bigrams, but the process as outlined is the basic idea. We still have one final data challenge: our vectors are incredibly sparse. Each sample is composed of a vector containing 7 1’s and thousands of 0’s. Here we turn to SciPy’s csr_matrix, specifically designed to store only the non-zero values of matrix and fully compatible with scikit-learn algorithms. . Results . While I considered many possible modeling techniques, I focused on support vector machines for two reasons. First, the compressed sparse row format of the one-hot encoded matrix offered significant memory and computation speed advantages given an algorithm that could work with this sparsity. Ultimately, discriminative algorithms met this criteria while generative models like LDA required the full dataset to parameterize its Gaussian distributions. Second, SVMs are simple and given their high performance, there was no need to delve into more complex options that could be harder to maintain. . SVMs offer high degrees of flexibility by using the kernel trick to transform the data space and increase class separation. Due to the high dimensionality of the data, classes were likely to be close to separable and empirical testing with non-linear kernels did not improve fit. Data scaling also negatively impacted scores in cross-validation. . On a held out test dataset, the SVM had an accuracy of 97.6% and an F1 score of 98%. This is pretty impressive and reminds us that reaching for simple tools first can save us a lot of time and compute. . I was curious what patterns I could find in the test samples that assigned incorrect labels. Of the 12,137 samples used in the test set, the SVM misclassified 293 of them. For 69 of these samples, the center bigram was not present in the training dataset. This is important since the model was predicting whether a separation exists between these two characters, but each of these bigrams would be assigned the same “unknown” placeholder value under the feature construction process. This is a general problem of using a fixed vocabulary that a more complex tokenization process could ameliorate. . Transformer Architectures . Turning to more modern methods, I flipped to Sebastian Ruder’s NLP Progress leaderboard for Chinese word segmentation. The top scoring paper on the Chinese Treebank 6 is “Towards Fast and Accurate Neural Chinese Word Segmentation with Multi-Criteria Learning” by Weipeng Huang et al. from Ant Group. Their methods combine a Chinese-language BERT model with additional projection layers, knowledge distillation, and quantization. I was more interested in seeing how a Transformer approach to the problem might differ, so I implemented a paired down version of this method. . The Ant Group paper also takes a different labeling approach, seeking to categorize each token as the beginning, middle, end, or single character of words in a sentence. To follow their scheme, the separated words in the training and test sets were concatenated to remove spaces, and each character was given a label in “B”, “M”, “E”, and “S” corresponding to beginning, middle, end, or single respectively. . I loaded a pre-trained BERT Encoder from Hugging Face and added a dropout and linear layer projecting to the space of possible classes. The labeling tokens were added to the vocabulary and the number of classes easily specified: . {python, eval=F} from transformers import BertTokenizerFast, BertForTokenClassification . tokenizer = BertTokenizerFast.from_pretrained( “bert-base-chinese”, cache_dir=cache_dir ) tokenizer.add_tokens([“A”, “S”, “B”, “M”, “E”]) . model = BertForTokenClassification.from_pretrained( “bert-base-chinese”, cache_dir=cache_dir ) model.classifier = torch.nn.Linear(768, 5, bias=True) model.num_labels = 5 . The model was fine-tuned on the provided segmentation dataset and constructed labels until convergence, as measured by the loss on a small validation set carved out of the training data. The BERT experiments offered many avenues for optimization and only a small subset were attempted for now. The main ablations focused on which parameters were trained; nested versions were run training just the linear classification layer, adding the last three BERT encoder layers, and training all parameters. The results demonstrated that including some encoding layers made a large difference. . I measured the Encoder classification model along two metrics; accuracy is the percent of test samples for which the encoder produced the correct label for every token, while “% Correct Tokens” is the percent of all tokens that received the correct label. My simple BERT implementation scored 74% accuracy and 97% on the “% Correct Tokens” metric. The model showed promising performance on this harder task and would likely benefit from additional ablations. I plan to work on improving training methods, swapping in more modern Encoders than BERT, and adding new projection layers. .",
            "url": "https://spencerbraun.github.io/blog/nlp/machine-learning/projects/pytorch/2021/07/20/chinese-segmentation-classic-and-modern-methods.html",
            "relUrl": "/nlp/machine-learning/projects/pytorch/2021/07/20/chinese-segmentation-classic-and-modern-methods.html",
            "date": " • Jul 20, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Selectively Editable Language Models",
            "content": "The post belows explains a project in editing language models I completed as part of Stanford’s course in natural language processing. If you want to skip to the point, the report and code are freely available. . Background: Taming Large Language Models . Large language models have been an area of exciting development in the last few years, and we now have applications and entire companies built around their improving performance and utility. The number of pretrained models hosted on Hugging Face has exploded, companies built on GPT-3’s API are rolling out betas, and GitHub Copilot amazes senior ML researchers (though not without controversies). All of these language models - from encoders like BERT variants, to decoders like GPT-3 or encoder-decoders like T5 and BART - rely on hundreds of hours of pretraining, unsupervised methods to endow the model parameters with familiarity in natural language’s syntax and semantics. While incredibly useful, this task also encodes specific factual knowledge in the model that remains static over time, despite changing facts on the ground in the real world. Fine-tuning these models on a specific task or dataset is quite common but is a costly and non-localized method for changing a specific fact learned during pretraining. Performance continues to improve in increasing model size, so any believer in the scaling hypothesis should only expect model alteration to become a more pressing problem (at least in the near future). . With this in mind, I set out to explore alternative methods of knowledge editing - ways in which we could update a specific fact and its context without degrading a model’s performance on unrelated language samples. Concretely, imagine a setting in which a model is deployed to generate sentences based on political news stories that occurred each day. If that model was pretrained on a text dataset created 5 years ago, a prompt like “New policies announced by the President,…” would likely fail to produce the correct name. . With the guidance of Eric Mitchell, a CS PhD student in the Stanford AI Lab, I extended an existing framework for editing deep learning models to the space of autoregressive transformer decoders. . Editable Neural Networks . Selectively editing neural network outputs is not novel, though methods investigated vary widely in their approaches and implementation. One paper that seemed especially relevant was “Editable Neural Networks,” published by Sinitsin et al. as a conference paper at ICLR 2020. They view editing through an error-correction lens and propose a training approach that can reliably alter mistaken network output while minimizing overall disturbance and computational requirements relative to other approaches. With their technique termed “Editable Training,” the authors employ meta-learning to push model parameters towards specific objectives. The model should be fine-tuned on the specific base learning task at hand (eg. minimize cross entropy on a language dataset or classification error on ImageNet) while also learning to be adaptable when we change the gold label for a given example. Once the model has been trained with this procedure, it should be primed for quick edits to its outputs using just a few gradient steps. Sinitsin et al. explore image classification and neural machine translation use cases but do not touch on language model settings. . Meta-Learning: Learning to Learn . For many practitioners familiar with machine learning but without prior experience with meta-learning, it can be an unintuitive concept to grasp. In vanilla neural network training, an objective is specified by a loss function, the loss is evaluated over some training samples, and the parameters are updated by small steps in a direction that decreases this loss by some form of gradient descent. In meta-learning, there are multiple objectives and the model parameters are trained to be adaptable to several, sometimes competing, tasks. Instead of evaluating the training success over a concrete metric, we are endowing the model with the ability to learn faster in the future. . I specifically focused on a type of meta-learning known as MAML - model-agnostic meta-learning - introduced by Finn et al. in 2017. As they describe in the problem set-up: . The goal of few-shot meta-learning is to train a model that can quickly adapt to a new task using only a few data points and training iterations. To accomplish this, the model or learner is trained during a meta-learning phase on a set of tasks, such that the trained model can quickly adapt to new tasks using only a small number of examples or trials. In effect, the meta-learning problem treats entire tasks as training examples. . What does this look like in practice? There are some excellent tools available for MAML, and I found Facebook’s PyTorch add-on Higher to be quite easy to use. It allows us to grab the model parameters, compute intermediate gradients, and take some gradient steps functionally. Important to understand is the distinction between the “inner loop” and “outer loop” of MAML. I found the description provided by Zintgraf et al. in “Fast Context Adaptation via Meta-Learning” quite clear: . MAML is trained with an interleaved training procedure, comprised of inner loop and outer loop updates that operate on a batch of related tasks at each iteration. In the inner loop, MAML learns task-specific network parameters by performing one gradient step on a task-specific loss. Then, in the outer loop, the model parameters from before the inner loop update are updated to reduce the loss after the inner loop update on the individual tasks. Hence, MAML learns a model initialisation that can generalise to a new task after only a few gradient updates at test time. . In code, the training process looks like: . {python eval=FALSE} . init an “outer loop” optimizer for the total loss . opt = torch.optim.Adam(model.parameters(), lr=1e-5) . loop over data batches containing a base objective example and meta-learning task example . for train_step, (base_example, meta_example) in enumerate(dataloader): . # init an &quot;inner loop&quot; optimizer for meta-learning gradient steps inner_opt = torch.optim.SGD(model.parameters(), lr=1e-3) # higher takes in model and optimizer with higher.innerloop_ctx( model, inner_opt, copy_initial_weights=False, # by not copying weights, we directly alter the model parameters track_higher_grads=True ) as (fmodel, diffopt): #returns functional model and optimizer # specify number of gradient steps in meta-learning objective for _ in range(num_grad_steps): # calculate loss on meta-learning objective loss = fmodel(meta_example).loss # take an optimizer step diffopt.step(loss) edit_loss = fmodel(meta_example).loss # calculate loss on base objective base_loss = model(base_example).loss # backprop and optimizer step total_loss = base_loss + alpha * edit_loss total_loss.backward() opt.step() opt.zero_grad() . In the snippet above, you can get a sense of how MAML is implemented in practice. For more on MAML and meta-learning, I highly recommend this excellent blog post by Lillian Weng. . Experiments . I focused on a single toy setting - altering the names associated with a person in a given context. My approach incorporated several distinct steps to build and evaluate what I termed the “language editing” procedure. First, I constructed a novel dataset consisting of an unedited passage, a sentence of the passage with a permuted named entity, and a record of the named entity changed. Second, a MAML training architecture was written allowing for optimization over both a typical language model cross-entropy loss along with an “adaptability” loss. Finally, performance measures were created to understand how the language editing procedure altered model parameters and highlight areas for future improvement. . Data . The dataset was generated using WikiText-103, available on the Hugging Face datasets hub. I used a SpaCy named entity recognition model to collect all named persons and create pairs of original sentences and copies with swapped names. . Model . I used a pretrained DistilGPT2 model, an autoregressive transformer language model with fewer parameters than a typical GPT-2 model. This was chosen out of practicality, as MAML stores a copy of the model parameters in memory. . Objectives . Following the example set by Sinitsin et al., I evaluated three losses that were weighted and backpropagated through the network. In the inner-loop optimization, the model parameters were pushed to learn the edited named entity via a cross-entropy loss. In the outer-loop optimization, the edit loss is defined as the cross-entropy of the altered MAML model on the edited sentence. The base loss is the original model’s cross-entropy on the unedited passage. Finally a locality loss is imposed by taking the KL divergence between the probabilities output by the original and meta-learned model on the same passage. This attempts to preserve the meta-learned model’s performance on unedited samples. For more details, I suggest reading section 3 of the report. . . Results: Locality is Key . After several ablations, I found that this training procedure could allow a pretrained decoder to pretty effectively incorporate a simple edit with minimal change in overall perplexity on the validation set. This result was especially promising given the lack of success when the edit was applied to a simply fine-tuned model. . However, the result leads to obvious questions about the practicality of the experimental setting. I attempted to have the language model update on a single edit after the MAML procedure, but in a deployed model we likely would want to make hundreds of edits each month. I have been performing more research on varied editing settings and hope to find a robust methodology. . One lesson that I found to be key in this project is the importance of minimizing model degradation on other, non-edited samples. There were many hyperparameter settings that allowed for successful output editing, but many came with a high cost to the overall perplexity of the model. Looking at the actual language produced by these degraded models demonstrated that even small changes in parameters could render the models useless. . This all suggests that model editing is a rich area of research with many problems yet to be solved. I encourage you to check out the project report and code if interested! .",
            "url": "https://spencerbraun.github.io/blog/nlp/machine-learning/pytorch/python/meta-learning/projects/2021/07/14/selectively-editable-language-models.html",
            "relUrl": "/nlp/machine-learning/pytorch/python/meta-learning/projects/2021/07/14/selectively-editable-language-models.html",
            "date": " • Jul 14, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://spencerbraun.github.io/blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://spencerbraun.github.io/blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://spencerbraun.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://spencerbraun.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}